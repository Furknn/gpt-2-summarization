{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x9DOgne5MJ9Z"
   },
   "outputs": [],
   "source": [
    "!pip install bert-extractive-summarizer\n",
    "!pip install transformers\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "nlp_folder_path = '/content/drive/MyDrive/Natural Language Processing/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hDLd231Su-vO"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "count = 0\n",
    "dataset = []\n",
    "for fj in os.listdir(nlp_folder_path+\"dataset\"):\n",
    "  with open(nlp_folder_path+ \"dataset/\" + fj, encoding=\"utf8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "    if data[\"Başlık\"]!=\"\" and data[\"Özet\"]!=\"\" and data[\"Metin\"]!=\"\":\n",
    "      instance = {\"id\":fj[:-5],\"title\":data[\"Başlık\"],\"abstract\":data[\"Özet\"],\"text\":data[\"Metin\"]}\n",
    "      dataset.append(instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FGN0zE9VbHuE"
   },
   "outputs": [],
   "source": [
    "processed=[ a[15:-5] for a in os.listdir(nlp_folder_path+\"summary_results_2\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qJcc3V1NbwF_"
   },
   "outputs": [],
   "source": [
    "processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iGnSn0abM9Cv"
   },
   "outputs": [],
   "source": [
    "from posixpath import split\n",
    "from summarizer import TransformerSummarizer\n",
    "import time \n",
    "\n",
    "GPT2_model = TransformerSummarizer(transformer_type=\"GPT2\",transformer_model_key=\"gpt2-large\")\n",
    "\n",
    "for data in dataset:\n",
    "\n",
    "  if data[\"id\"] not in processed:\n",
    "    start = time.time()\n",
    "    \n",
    "    print(data[\"id\"])        \n",
    "    body = data[\"text\"][:1000000]\n",
    "    \n",
    "    # ''.join(data[\"text\"].split(\" \")[:10000])\n",
    "    \n",
    "    full = ''.join(GPT2_model(body, min_length=60,max_length=len(data[\"abstract\"])))\n",
    "    output = {\"id\":data[\"id\"],\"summary\":full}\n",
    "\n",
    "    # Save the results in same folder as the preprocessed dataset\n",
    "    if not os.path.exists(nlp_folder_path+\"summary_results_2\"):\n",
    "        os.makedirs(nlp_folder_path+\"summary_results_2\")\n",
    "    with open(nlp_folder_path+\"summary_results_2/summary_result_{}.json\".format(data[\"id\"]), \"w\", encoding=\"utf8\") as f:\n",
    "        json.dump(output, f, ensure_ascii=False)\n",
    "\n",
    "    print(\"Summary for document {} are extracted\".format(data[\"id\"]))\n",
    "    print(\"Time elapsed: {}\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7IceZ9xsS6AG"
   },
   "outputs": [],
   "source": [
    "!pip install rouge\n",
    "import rouge\n",
    "import os,json\n",
    "\n",
    "def compare_rouge(hypothesis, reference):\n",
    "    scorer = rouge.Rouge()\n",
    "    scores = scorer.get_scores(hypothesis, reference)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5fDo5md1TBvV"
   },
   "outputs": [],
   "source": [
    "# list dir summary_results_2\n",
    "filenames = os.listdir(nlp_folder_path+ 'summary_results_2')\n",
    "article_ids = []\n",
    "# for each file in dir do substr [15:-5]\n",
    "for filename in filenames:\n",
    "    article_ids.append( filename[15:-5])\n",
    "\n",
    "for article in article_ids:\n",
    "    summary ={}\n",
    "    reference = {}\n",
    "\n",
    "    # read summary_results_2/summary_result_{}.json\n",
    "    with open(nlp_folder_path+'summary_results_2/summary_result_{}.json'.format(article), 'r') as f:\n",
    "        summary = json.load(f)\n",
    "\n",
    "    # read dataset/{}.json\n",
    "    with open(nlp_folder_path+ 'dataset/{}.json'.format(article), 'r') as f:\n",
    "        reference = json.load(f)\n",
    "\n",
    "    # compare_rouge(summary, reference)\n",
    "    scores = compare_rouge(summary['summary'], reference['Özet'])\n",
    "\n",
    "    summary['scores'] = scores\n",
    "\n",
    "    # write summary_results_2/summary_result_{}.json\n",
    "    with open(nlp_folder_path+'summary_results_2/summary_result_{}.json'.format(article), 'w') as f:\n",
    "        json.dump(summary, f)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
